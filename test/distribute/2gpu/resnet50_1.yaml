apiVersion: elastic.pytorch.org/v1alpha1
kind: ElasticJob
metadata:
  name: imagenet50-1
  namespace: elastic-job
spec:
  # Use "etcd-service:2379" if you already apply etcd.yaml
  rdzvEndpoint: etcd-service:2379 #"<your_etcd_endpoint>:<your_etcd_port>"
  minReplicas: 1
  maxReplicas: 2
  replicaSpecs:
    Worker:
      replicas: 2
      restartPolicy: ExitCode
      template:
        apiVersion: v1
        kind: Pod
        metadata:
          labels:
            "sharedgpu/gpu_request": "2.0"
            "sharedgpu/gpu_limit": "2.0"
            "sharedgpu/priority": "100"
            "sharedgpu/group_name": "imagenet50-1"
            "sharedgpu/min_available": "2"
        spec:
          schedulerName: kubeshare-scheduler
          containers:
            - name: elasticjob-worker
              image: torchelastic/examples:0.2.0
              #imagePullPolicy: Always
              env:
              - name: NCCL_IB_DISABLE
                value: "1"
              - name: NCCL_P2P_DISABLE
                value: "1"
              args:
                - "--nproc_per_node=2"
                - "/workspace/examples/imagenet/main.py"
                - "--arch=resnet50"
                - "--epochs=5"
                - "--batch-size=32"
                # number of data loader workers (NOT trainers)
                # zero means load the data on the same process as the trainer
                # this is set so that the container does not OOM since
                # pytorch data loaders use shm
                - "--workers=0"
                - "/workspace/data/tiny-imagenet-200"
                  #resources:
                  #limits:
                  #nvidia.com/gpu: 1
